---
title: 'Ethics in Computing in the Age of Generative AI'
date: 2024-09-20 00:00:00
description: This blog to evaluate and discuss Ethics in Computing in the age of Generative AI
featured_image: '/images/demo/AI.jpg'
---

![](/images/demo/AI.jpg)

## Reflection on Ethics in Computing in the Age of Generative AI

### Introduction

The rise of generative AI technologies that can create text, images, and even code has brought new ethical challenges to the forefront of computing. These challenges impact developers and users and extend into broader societal concerns around transparency, fairness, accountability, and privacy. This reflection will explore these ethical dimensions using Rolfe et al.'s (2001) reflective model, structured as What? So What? Now What?, and incorporating insights from relevant academic and industry sources.

### Ethical Dilemmas Posed by Generative AI

In recent years, generative AI has transformed various industries, from content creation to customer service and even cybersecurity. While these technologies offer immense potential, they also raise significant ethical concerns. According to the analysis by Corrêa et al. (2023), transparency, accountability, and fairness are some of AI's most critical ethical principles. These concerns are viewed in the Deckard (2023) article, which highlights the difficulty in ensuring that AI systems behave in ways that are fair, unbiased, and transparent.

In my experience as an IT professional working on AI-driven applications, I have witnessed how generative AI models can produce content that closely mimics human creativity but may inadvertently reinforce harmful stereotypes or propagate misinformation. For example, models trained on biased datasets can produce outputs that reflect those biases, raising questions about the ethical use of AI in decision-making processes. This challenge is further compounded by the black-box nature of many AI systems, where the reasoning behind outputs is opaque, even to developers. As a result, it becomes challenging to hold developers accountable for unintended consequences, such as discriminatory outcomes or privacy violations.

Moreover, the global review of AI guidelines by Corrêa et al. (2023) shows that while there is a growing consensus on the need for ethical AI, most guidelines are non-binding, meaning they lack the enforcement mechanisms needed to ensure ethical AI deployment. 

### The Importance of Transparency, Accountability, and Fairness

The ethical implications of generative AI extend beyond technical issues and touch on broader societal concerns, such as trust, accountability, and inclusivity. In reflecting on these concerns, the lack of transparency in AI decision-making is particularly troubling. As the Deckard (2023) article highlights, generative AI systems often operate as black boxes, making it difficult for users and regulators to understand how decisions are made. This opacity can lead to a loss of trust in AI systems, particularly in high-stakes domains such as healthcare or finance, where the consequences of biased or erroneous outputs can be severe (Floridi and Cowls, 2022).

This lack of transparency also raises questions about users' autonomy in interacting with AI systems. According to Binns (2018), autonomy in AI is not just about how much control users have but also about their ability to understand and meaningfully engage with the AI systems they use. In many cases, generative AI models produce outputs that users cannot fully explain or critique, leading to a disempowerment of the user. This raises ethical concerns about whether AI systems promote or undermine individuals' autonomy.

Accountability is another critical issue. Generative AI systems can produce content that may infringe on intellectual property rights, perpetuate harmful stereotypes, or generate harmful content (Loi, 2020). However, when these ethical breaches occur, who should be held responsible often needs to be clarified. Should it be the developers, organizations deploying the systems, or AI? The Deckard (2023) and Corrêa et al. (2023) both point out that there is a lack of clarity on where responsibility lies, especially in situations where AI models are trained on large datasets collected from multiple sources.

Furthermore, generative AI models frequently challenge the ethical principle of fairness. These models learn from vast datasets often scraped from the internet without proper curation or checks for bias. Consequently, the models may perpetuate societal biases regarding race, gender, and other sensitive characteristics (Whittaker et al., 2018). As O’Neil (2016) argues in her seminal work Weapons of Math Destruction, unchecked AI systems can reinforce existing inequalities rather than alleviate them. 

### Moving Towards Ethical Governance and Solutions

The need for a more structured and enforceable approach to AI ethics governance is essential to address these challenges. While voluntary guidelines exist, as discussed by Corrêa et al. (2023), the industry requires a combination of regulatory measures, transparent development practices, and public awareness campaigns to ensure that AI technologies serve the broader good.

Firstly, developers and organizations deploying AI systems must adopt transparent development practices. One solution to the transparency problem is using explainable artificial intelligence (XAI) techniques, which aim to make the decision-making processes of AI systems more interpretable to users. XAI could provide users with explanations of how AI models reach their conclusions, thereby fostering trust and accountability (Gunning et al., 2019). However, as XAI is still in its infancy, further research and development are required to be applied effectively across different domains.

In addition to transparency, there is an urgent need to create mechanisms for ethical accountability. Governments and international bodies must work together to establish clear guidelines on who is responsible for the actions of AI systems. This may involve the development of new legal frameworks that allocate responsibility to both developers and organizations using AI systems (Loi, 2020). For example, auditability of AI systems, which involves creating records of decision-making processes, could help ensure that developers and companies are held accountable for their outputs.

Moreover, addressing the issue of fairness requires a more deliberate effort to curate training datasets and implement fairness-aware algorithms. This means identifying and mitigating biases in training data and ensuring that AI models undergo rigorous testing to ensure they do not produce discriminatory outputs. Initiatives such as Algorithmic Fairness (Binns, 2018) and the Fairness, Accountability, and Transparency in Machine Learning (FAT/ML) community provide frameworks for developing more equitable AI systems. 

Lastly, raising public awareness about the ethical challenges of generative AI is essential. Users need to be educated about the potential risks of interacting with AI systems, from privacy concerns to spreading misinformation. Public education campaigns and transparent AI policies can help users make informed decisions about their interactions with AI technologies (Floridi and Cowls, 2022).

### Conclusion

Reflecting on the ethical issues surrounding generative AI through Rolfe et al.’s reflective model has underscored the importance of addressing transparency, accountability, and fairness in developing and deploying these systems. While generative AI holds great promise, it poses significant ethical challenges requiring urgent attention. Combining regulatory efforts, technical advancements in explainability and fairness, and public education will be essential to ensure that AI technologies are deployed responsibly and ethically.

### References 
Binns, R., 2018. Fairness in machine learning: Lessons from political philosophy. Proceedings of the 2018 Conference on Fairness, Accountability, and Transparency, pp.149-159.
Corrêa, N.K., Galvão, C., Santos, J.W., Del Pino, C., Pinto, E.P. and Barbosa, C., 2023. Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance. Patterns, 4, p.100857.
Floridi, L. and Cowls, J., 2022. The ethics of artificial intelligence: Principles, challenges, and opportunities. Nature Machine Intelligence, 4(3), pp.151-157.
Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S. and Yang, G.Z., 2019. XAI—Explainable artificial intelligence. Science Robotics, 4(37), p.eaay7120.
Loi, M., 2020. Ethics of artificial intelligence: Prospects and challenges. Philosophy & Technology, 33(2), pp.209-224.
O'Neil, C., 2016. Weapons of math destruction: How big data increases inequality and threatens democracy. Crown Publishing Group.
Rolfe, G., Freshwater, D. and Jasper, M., 2001. Critical reflection for nursing and the helping professions: A user’s guide. Palgrave Macmillan.
Whittaker, M., Crawford, K., Dobbe, R., Fried, G., Kaziunas, E., Mathur, V., ... and Schwartz, O., 2018. AI now report 2018. AI Now Institute at New York University.


---
